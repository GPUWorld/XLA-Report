# 组间交流

## 与NNVM组交流

* 首先了解了NNVM与TVM，Mxnet等相关知识。NNVM 编译器是用于中间计算图表示和优化计算图的一种工具，建立在 TVM 堆栈的两个组成的基础上：用于计算图的 NNVM (狭义 NNVM)，和用于张量计算的 TVM。TVM 中间低层计算图 IR 表示采用的是 Halide IR 图思想。
* TVM走了和目前的XLA比更加激进的技术路线，tvm可以用来使得实现XLA需要的功能更加容易 ：已有的解决方案本身基于高级图表示的规则变换，可以产生一些图级别的组合op优化，如conv-bn fusion，但是依然要依赖于手写规则来达到从图的表示到代码这一步。图的op表示到代码本身可以选择的东西太多，如何做线程，如何利用shared memory，而大部分没有在图语言里面得到刻画，导致难以自动化。 这样下去深度学习系统的瓶颈必然从op实现的复杂度变成了实现graph compiler中模式生成规则的复杂度。走这个方向需要非常大的工程团队的支持，而我们希望采用更少的人力达到同样甚至更好的效果。
* tvm 在 CPU 上的优化需要借助 llvm 作为基础，在 llvm 的优化基础之前，tvm 针对自身计算图给出了两点重要的优化，包括提高cache命中率与使用SIMD等向量化并行化的方法；在GPU上TVM主要针对卷积(convolution)操作进行了优化。
* 与TVM不同的是，XLA 有一套同时适用于多个资源架构的优化方案；而TVM 对计算图的优化方案主要是针对不同硬件资源具有不同的优化方案。XLA对于计算图的优化包含死代码删除、公共子表达式提取等常规的优化；在异构方面，XLA的一种手段是生成LLVM IR，将优化交给LLVM来解决。
* NNVM组对于NNVM及TVM的优化进行了非常精确的测试，从结果看TVM的cpu对于给定样例的优化效果都非常可观。



## 与Darkroom组交流

* 首先了解了Darkroom的相关概念。Darkroom是图像处理领域的一门编程语言，其编译器提供将图像处理算法编译为高效硬件的功能。Darkroom组主要对Darkroom的原理、实现，和其他语言的对比、应用进行调研与测试。
* 在对计算图的优化方面，Darkroom和XLA等具有类似的手段，如常量传播，表达式强度削弱等；Darkroom还有特殊的流水线行缓存，对卷积有一些特殊优化。
* Darkroom与NNVM组的TVM和本组的XLA的最大区别是，Darkroom还针对ASIC和FPGA等可重构硬件平台。
* XLA对于计算图的优化包含死代码删除、公共子表达式提取等常规的优化；在异构方面，XLA的一种手段是生成LLVM IR，将优化交给LLVM来解决。

